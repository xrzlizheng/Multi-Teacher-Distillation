# 自适应多老师蒸馏 - 让监督学习更上一层楼
## 一个能把多个预测模型变成轻量级高性能学生模型的神奇方法

可以看看我的博客https://blog.csdn.net/qq_36603091/article/details/146582991

在监督学习中，用单个预测模型（比如XGBoost、LightGBM或*Random Forest*）是常规操作。但俗话说得好，三个臭皮匠顶个诸葛亮，把多个模型结合起来往往能大幅提升性能。传统方法是用固定权重或逻辑回归来混合多个模型的预测结果，这就像给每个模型发了一样的工资，不管它们干得好不好。这种方法虽然简单，但错过了让每个模型在特定情况下大显身手的机会。![img](https://i-blog.csdnimg.cn/direct/76d849bc7c6d4d62981b26b05d5c79c9.png)

### 自适应多老师蒸馏

为了解决这个问题，我们提出了一个新方法：与其用固定的方式混合，不如用一个轻量级神经网络“学生”来同时向多个复杂的“老师”模型学习。每个老师（比如XGBoost或LightGBM）都会给出预测概率，学生在训练时就会用到这些概率。关键是，我们的学生网络还会学习注意力权重，动态决定每个老师在每个特定预测中的影响力。举个例子，当预测客户是否会响应促销活动时，学生可能会更依赖XGBoost对年轻客户的预测，而对老年客户则更信任随机森林的预测。

### 优势？

首先，通过允许每个预测的动态加权，学生模型可以灵活调整自己，捕捉到单一模型或静态集成无法捕捉的复杂模式。

其次，蒸馏让学生模型能够将所有老师的宝贵知识内化到一个紧凑的神经网络中，提供更高的准确性和更好的泛化性能。

这种方法最终产生了一个非常高效和紧凑的最终模型，非常适合那些速度、可解释性和准确性同样重要的部署场景。